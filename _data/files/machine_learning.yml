
- h2: Machine learning
  question: What is the difference between data science and machine learning
  answer: >
    Data science = statistical analysis + data engineering in order to get 
    information from the data
    <br><br>
    Machine learning = create a model to predict something. You can also define 
    it as teaching computers to learn by examples

- question: What are the typical steps of a machine Learning process?
  answer: > 
    The typical steps of a machine learning process are
    <br>- domain knowledge
    <br>- feature selection
    <br>- choose machine learning algorithm
    <br>- training
    <br>- evaluation
    <br>- optimization
    <br>- testing
    <br>- deployment
   

- question: What is overfitting and what are some techniques for preventing it?
  answer: >
    Overfitting happens when your model works well with the training dataset but
    does not work well with new data, which means the model cannot generalize well. 
    <br>
    To prevent overfitting you can for example use regularization and cross-validation.

- question: What is cross-validation in machine learning?
  answer: > 
    Cross-validation refers to the process of splitting a dataset into a 
    training set (usually 80%) and test set (usually 20%) and evaluate
    the performance of the model with those two datasets. This process
    is usually repeated 10 times.

- question: What is regularization?
  answer: >
    Regularization adds a penalty on the different parameters of the model to reduce the 
    freedom of the model. Regularization is usually done by adding a correction term to 
    the formula for a mathematical model. This correction term penalizes the higher order 
    terms so that the model is forced to be simpler.

- question: What is the curse of dimensionality?
  answer: >
    The curse of dimensionality is a tendency that it is easier to overfit a dataset 
    when there are few points and many features. Data needs to increase exponentially 
    with the number of features in order not to have overfitting.

- question: What is unsupervised learning? Make also some examples
  answer: > 
    Unsupervised learning is a machine learning process with unlabeled data.
    <br><br>
    Some examples of unsupervised learning are
    <br> - Clustering algorithms (K-means, hierarchical custering, Probabilistic clustering)
    <br> - Dimension reduction algorithms (PCA, Single Value Decomposition SVD)

- question: What is reinforcement learning
  answer: >
    It is a machine learning algorithm that receives feedback on his output so that the accuracy
    of the output is improved based on this feedback. Finally the algorithm learns through trial
    and error.

- question: What is supervised learning
  answer: It is a machine learning process with a labeled training dataset.

- question: What are the two main types of supervised learning algorithms?
  answer: >
    Supervised machine learning algorithms are mainly classified 
    into classification and regression.

- question: What are hyperparameters?
  answer: >
    Hyperparameters are parameters used by the training model and 
    they must be obviously set before starting the training process. 
    An example is the the k value in the k-nearest neighbor algorithm.


- h3: Evaluate machine learning methods
  question: What is bias in a machine learning model?
  answer: >
    Bias is the error that is introduced by analyzing a complex model using a too simple model. 
    In other words bias is the inability of the model to capture the real nature of the true 
    relationship in the data.

- question: What does high bias mean for my model?
  answer: High bias means the model is underfitting the data.

- question: What is the variance of a machine learning model?
  answer: > 
    Variance is the amount that the estimate of the target function will change if different training 
    data was used.

- question: What does high variance mean for my model?
  answer: > 
    High variance means the model is overfitting, ie small changes in the training data set lead 
    to big changes in the output of the model.


- question: What matrix do you construct to evaluate the performance of a classifier?
  answer: >
    To evaluate the performance of a classifier you construct a confusion matrix
    <table>
      <tr>
        <td></td>
        <th>y = 1</th> 
        <th>y = -1</th>
      </tr>
      <tr>
        <td>y# = 1</td>
        <td>TP</td> 
        <td>FP</td>
      </tr>
      <tr>
        <td>y# = -1</td>
        <td>FN</td> 
        <td>TN</td>
      </tr>
    </table>
    <br>
    where 
    y = actual label, y# = predicted label, TP = true positives, FP = false positives, FN = false negatives
    TN = true negatives.
    <br><br>
    True positives (negatives) = number of items with label y = 1 (-1) that 
    are classified correctly.
    <br><br>
    False positives (negatives) = number of items with labels y = -1 (1) that
    are classified wrongly.

- question: Can you mention some numerican quantities used to evaluate classifiers?
  answer: >
    accuracy = (TP + TN) / n
    <br>true positive rate or sentitivity = TP / (TP + FN)
    <br>true negative rate or specificity = TN / (TN + FP)
    <br>Precision  = TP / (TP + FP)
    <br>False positive rate (FNR)  = FP / (TN + FP)
    <br>False  negative rate (FNR)  = FN / (TP + FN)
    <br><br>
    Accuracy is the percentage of the correctly classified items.
    <br><br>
    Precision is the percentage of the items that were correctly classified
    as positive.

- question: To evaluate a generic machine learning model what numerical quantity do you typically use?
  answer: You would the use misclassification error or accuracy

- question: To evaluate a machine learning model for medicine what numerical quantities do you typically use?
  answer: Doctors would use sensitivity and specificity.

- question: If you want to evaluate a classifier with a curve what do you use?
  answer: >
    You can use an ROC curve. An ROC curve answers to the question 
    â€œfor a particular false positive rate what is the corresponding true positive rate?
  image: roc_curve.png

- h3: Naive Bayes
  question: What is Naive Bayes?
  answer: >
    Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem that is used
    to solve classification problems.

- question: Why is Naive Bayes called "naive"?
  answer: >
    Because the Naive Bayes is based on the idea that the predictor variables are independent of 
    each other while in nature this is oft not the case.

- question: What is the Bayes theorem?
  answer: >
    The Bayes theorem is a mathematical formula for finding P(A|B) from P(B|A).
    The formula is 
    <br><br>
    P(A|B) = P(B|A) * P(A) / P(B)


- question: What are the main advantages of Naive Bayes?
  answer: >
    The Naive Bayes is a relatively simple and very quick algorithm since it is a probabilistic 
    model that does not require a training step. This makes it very scalable.

- question: When is the Naive Bayes usually used?
  answer: > 
    Naive Bayes is often used in <br>
    - text classification like spam filter<br>
    - real-time classification since it is fast<br>
    - multi-class prediction

- question: When Naive Bayes is used with numerical variables, what condition is assumed on the data?
  answer: >
    Numerical variables used in Naive Bayes are expected to have a normal distribution.
    This is not always the case in practice.

- question: How does the Naive Bayes perform with categorical variables?
  answer: > 
    It performs well with categorical variables since no assumpion is made on
    the data distribution. By contrast with numerical variables a normal distribution
    is assumed.

- h3: Regression
  question: What do you predict with linear regression?
  answer: Linear regression predicts a real value

- question: What are the most common techniques used for computing the coefficients in linear regression?
  answer: >
    The most common techniques used for computing the coefficients in linear regression are 
    <br> - Ordinary Least Squares. Seeks to minimize the sum of the squared residuals, ie the distance 
    between the points and the regression line.
    <br><br> - Gradient descendent. Works by inizializing randomly the coefficients. A learning rate is 
    used as a scale factor and the coefficients are updated in the direction towards minimizing the error. 
    The process is repeated until a minimum sum squared error is achieved or no further improvement is 
    possible.

- question: What is logistic regression?
  answer: >
    It is a statistical technique that is used to analyze a dataset and predict the binary outcome. 
    The outcome has to be a binary outcome that is either zero or one or a yes or no. 
    In other words the output of a logistic regression is always categorical (= discrete)


- question: What do you predict with logistic regression?
  answer: >
    Logistic regression predicts the probabilty that a variable has one value or another. 
    Thus it is a binary variable.

- question: What is the KNN algorithm?
  answer: >
    A k-nearest neighbor (KNN) method scores an example by finding the k training examples 
    nearest to the example and then taking the average of their outcomes as the score.

- question: Can you mention some advantages and disadvantages of the KNN algorithm?
  answer: >
    Some advantages are
    <br>- simple concept
    <br>- builiding a model is cheap
    <br>- no assumption is made about the data distrubution
    <br><br>Some disadvantages are
    <br>- classifying unknown records is expensive, in particular with big data sets, 
         since you have to find the K nearest neighbors
    <br>- pretty affected by data missing and data scaling

- h3: Decision trees
  question: What is a decision trees?
  answer: >
    A decision tree is a classification algorithm that models the features and the data in a tree-like 
    structure.
- question: How are decision trees built?
  answer: >
    Building a decision tree is a recursive process. The dataset is split into subsets, which are 
    then split repeatedly into even smaller subsets, and so on and so forth until the process stops 
    when the algorithm determines the data within the subsets are sufficiently homogenous, or another 
    stopping criterion has been met.
- question: What is a great advantage of using decision trees?
  answer: >
    The output of the decision trees is human readable. This makes thiis algorithm interesting to be 
    used in cases where the decision mechanism needs to be transparent because of legal reasons 
    (for example medical diagnostics)
- question: What are some disadvantages of decision trees?
  answer: >
    - each split considers one feature at a time which means that the decision lines are basically 
    parallel to the axis and are not diagonal, which would be the case if you would evaluate two or 
    more features within the same split.
    <br><br>- it is easy to overfit or underfit the model
    <br><br>- small changes in the input data will lead to a very different output
- question: >
    What technique do you use to reduce the overfitting of a decision tree. How many appraches there are?
  answer: >
    To reduce the overfitting of a decision tree you can use pruning, which reduces the size of the tree. 
    There are two approaches to pruning<br>
    <br><br>Pre-pruning - the tree stop growing when it reaches a certain number of decisions or when 
    the decision nodes contains too few items. A disadvantage of this approach is that some important 
    data could be pruned.
    <br><br>Post-pruning - the tree can grow as needed and only at the end it is checked if the tree 
    is too big. This approach guarantees that the important data is not pruned.

- question: >
   What are the main techniques to combine decision trees to obtain a more accurate model 
   (so called ensemble techniques)?
  answer: >
    There are two techniques
    <br><br> Boosting algorithms. This is a sequential process. In boosting algorithms learners 
    are learned sequentially with early learners fitting simple models to the data and then analyzing 
    data for errors. After each training step the weights are redistributed. 
    Misclassified data gets an increased weight so that this data will be more on focus in the 
    next train step.
    <br><br>Bagging techniques. This is a parallel process. In a bagging technique the dataset is 
    divided into n samples using randomized sampling.Then a model is build on each sample. 
    After that the resulting models are combined using voting or averaging.