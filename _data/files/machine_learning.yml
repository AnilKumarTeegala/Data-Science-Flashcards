
- h2: Machine Learning

  question: What is the difference between data science and machine learning
  answer: >
    <b>Data science</b> = statistical analysis + data engineering in order to get 
    information from the data.
    <br><br>
    <b>Machine learning</b> = create a model to predict something. You can also define 
    it as teaching computers to learn by examples.

- question: What are the typical steps of the creation of a machine learning model?
  answer: > 
    The typical steps of a machine learning process are:
    <br><br>- domain knowledge
    <br><br>- feature selection
    <br><br>- choose machine learning algorithm
    <br><br>- training
    <br><br>- evaluation
    <br><br>- optimization
    <br><br>- testing
    <br><br>- deployment
   
- question: >
    All machine learning algorithms work essentially in a similar way. What is it?
  answer: >
    All machine learning algorithms work essentially like
    <br>- make a prediction
    <br>- compute the error given by (predicted values - real values)
    <br>- make a new prediction that reduces the error above
    <br>- repeat until the error is minimized

  question: What is bias in a machine learning model?
  answer: >
    <b>Bias</b> is the error that is introduced by analyzing a complex model using a too simple model. 
    In other words bias is the inability of the model to capture the real nature of the true 
    relationship in the data.

- question: What does high bias mean for a machine learning model?
  answer: High bias means the model is underfitting the data.

- question: What is the variance of a machine learning model?
  answer: > 
    <b>Variance</b> is the amount that the estimate of the target function will change if different training 
    data was used.

- question: What does high variance mean for a machine learning model?
  answer: > 
    High variance means the model is overfitting, i.e. small changes in the training data set lead 
    to big changes in the output of the model.


- question: What is overfitting and what are some techniques for preventing it?
  answer: >
    Overfitting happens when your model works well with the training dataset but
    does not work well with new data, which means the model cannot generalize well. 
    <br>
    To prevent overfitting you can for example use regularization and cross-validation.

- question: What is cross-validation in machine learning?
  answer: > 
    <b>Cross-validation</b> refers to the process of splitting a dataset into a 
    training set (usually 80%) and test set (usually 20%) and evaluate
    the performance of the model with those two datasets. 

- question: What is K-fold cross-validation?
  answer: >
    <b>K-fold cross-validation</b> is a kind of cross-validation. The steps are
    <br><br>- split a dataset into k parts. Each part is called fold.
    <br><br>- take one fold i as test set and take the remaining (k-1) folds as training set.
    <br><br>- train you classification algorithm with the two datasets from previous point. Finally for 
    each fold we get an accuracy value. i.e. in total we get K accuracy values.
    <br><br>- the final accuracy value for your classification algorithm is given by the average of
    the K accuracy values.

- question: What is regularization?
  answer: >
    <b>Regularization</b> adds a penalty on the different parameters of the model to reduce the 
    freedom of the model. Regularization is usually done by adding a correction term to 
    the formula for a mathematical model. This correction term penalizes the higher order 
    terms so that the model is forced to be simpler.

- question: What is the curse of dimensionality?
  answer: >
    The curse of dimensionality is a tendency that it is easier to overfit a dataset 
    when there are few points and many features. Data needs to increase exponentially 
    with the number of features in order not to have overfitting.

- question: What is unsupervised learning? Make also some examples of algorithms
  answer: > 
    Unsupervised learning is a machine learning process with unlabeled data.
    <br><br>
    Some examples of unsupervised learning are
    <br> - Clustering algorithms (K-means, hierarchical custering, Probabilistic clustering)
    <br> - Dimension reduction algorithms (PCA, Single Value Decomposition SVD)

- question: What is reinforcement learning?
  answer: >
    <b>Reinforcement learning</b> is a machine learning algorithm that receives feedback on its output so that the accuracy
    of the output is improved based on this feedback. In other words the algorithm learns through trial
    and error.

- question: What is supervised learning?
  answer: <b>Supervised learning</b> is a machine learning process with a labeled training dataset.

- question: What are the two main types of supervised learning algorithms?
  answer: >
    Supervised machine learning algorithms are mainly classified 
    into classification and regression.

- question: Can you mention some supervised machine learning algorithms?
  answer: >
    The most common supervised machine learning algorithms are 
    <br>K-nearest neighbor, Naive Bayes, Decision Trees, Linear Regression, 
    Support Vector Machines.

- question: What are hyperparameters?
  answer: >
    Hyperparameters are parameters used by the training model and 
    they must be obviously set before starting the training process. 
    An example is the the k value in the k-nearest neighbor algorithm.

- h3: Evaluate machine learning methods
  question: What is the difference between model evaluation and model validation?
  answer: >
    The difference between model evaluation and model validation is that
    <br><br><b>model evaluation</b> is used to check the performance of a machine learning model
    on the training set.
    <br><br><b>model validation</b> is used to ensure that a model performs well also on a test set.

- question: What matrix do you construct to evaluate the performance of a classifier?
  answer: >
    To evaluate the performance of a classifier you construct a confusion matrix
    <table>
      <tr>
        <td></td>
        <th>y = 1</th> 
        <th>y = -1</th>
      </tr>
      <tr>
        <td>y# = 1</td>
        <td>TP</td> 
        <td>FP</td>
      </tr>
      <tr>
        <td>y# = -1</td>
        <td>FN</td> 
        <td>TN</td>
      </tr>
    </table>
    <br>
    where 
    y = actual label, y# = predicted label, TP = true positives, FP = false positives, FN = false negatives
    TN = true negatives.
    <br><br>
    True positives (negatives) = number of items with label y = 1 (-1) that 
    are classified correctly.
    <br><br>
    False positives (negatives) = number of items with labels y = -1 (1) that
    are classified wrongly.

- question: Can you mention some metrics used to evaluate classifiers?
  answer: >
    Accuracy = (TP + TN) / n
    <br>true positive rate or sentitivity  or recall = TP / (TP + FN)
    <br>true negative rate or specificity = TN / (TN + FP)
    <br>Precision  = TP / (TP + FP)
    <br>False positive rate (FPR)  = FP / (TN + FP)
    <br>False  negative rate (FNR)  = FN / (TP + FN)
    <br><br>
    <b>Accuracy</b> is the percentage of the correctly classified items.
    <br><br>
    <b>Precision</b> is the percentage of the items classified as positive that are actually positive
    <br><br>
    <b>Sensitivity</b> or <b>Recall</b> is the percentage of the positive items that
    were actually classified as positive.
    <br><br>
    <b>Specificity</b> is the percentage of the negative items that
    were actually classified as negative.

- question: > 
    What metrics are more meaningful with class-balanced datasets and what work better 
    for class-unbalanced datasets? 
  answer: >
    When the dataset is class-balanced, the accuracy is a good metric. 
    <br><br>If the dataset is class-unbalanced, precision and recall
    are better metrices. 

- question: To evaluate a generic machine learning model what metrics do you typically use?
  answer: > 
    You would probably use the misclassification error or accuracy. 
    However, notice that accuracy works well when all classes are equally important and the 
    training dataset is balanced.

- question: > 
    To evaluate a machine learning model for cancer detection what metrics do 
    doctors typically use?
  answer: > 
    Doctors would use sensitivity and specificity because the cost of a misclassified cancer 
    is much higher than the cost of a healthy tissue classified as cancer. In addition a typical
    cancer dataset is strongly unbalanced, with few cancel samples and many healthy samples.

- question: What is the meaning and use of the F-measure (also called F-score)?
  answer: >
    The F-measure combines the precision and recall using the harmonic mean.
    <br><br>
    F-measure = 2 (Precision X Recall) / (Precision + Recall)
    <br><br>
    The F-measure value is between 0 (worst case) and 1 (best case). It can be used
    when you want to compare the performance of two classifiers w.r.t. precision and recall
    but you are not sure which of the two values to pick. So with the F-measure you seek
    a balance between precision and recall.



- question: If you want to evaluate a classifier with a curve what do you use?
  answer: >
    You can use an ROC curve. The ROC curve plots the sensitivity (= TPF) against
    1-specificity (=FPR) for different values of a parameter that affects the classifier.
    <br><br>
    Sensitivity = probability of predicting that a real positive will be positive. This can
    be seen as the accuracy of positives.
    <br>
    Specificity = probability of predicting that a real negative will be negative. This can
    be seen as the accuracy of negatives.
    <br><br>
    The best result is high sensitivity and low specificity which means the closest the curve
    is to point (0,1), the better. This can be also expressed as the farther the curve is
    from the diagonal line (0,0)-(1,1) the better. This diagonal line represents the line of 
    a classifier working at random. 
    <br>
    The distance of the curve from point (0,1) can be measured with the AUC (area under the curve)
    that should be close to 1 as much as possible.

    
    answers to the question 
    “for a particular false positive rate what is the corresponding true positive rate?
  image: roc_curve.png

- h3: Naive Bayes
  question: What is Naive Bayes?
  answer: >
    Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem that is used
    to solve classification problems.

- question: Why is Naive Bayes called "naive"?
  answer: >
    Because the Naive Bayes is based on the idea that the predictor variables are independent of 
    each other while in nature this is often not the case.
    <br><br>
    A Naive Bayes classifier calculates the probability of an item of belonging to each possible
    output class. The class with the highest probability is then chosen as output.

- question: What is the Bayes theorem?
  answer: >
    The Bayes theorem is a mathematical formula for finding P(A|B) from P(B|A).
    The formula is 
    <br><br>
    P(A|B) = P(B|A) * P(A) / P(B)


- question: What are the main advantages of Naive Bayes?
  answer: >
    The Naive Bayes is a relatively simple and very quick algorithm since it is a probabilistic 
    model that does not require a training step. This makes it very scalable.

- question: When is the Naive Bayes usually used?
  answer: > 
    Naive Bayes is often used in <br>
    - text classification like spam filter<br>
    - real-time classification since it is fast<br>
    - multi-class prediction

- question: When Naive Bayes is used with numerical variables, what condition is assumed on the data?
  answer: >
    Numerical variables used in Naive Bayes are expected to have a normal distribution.
    This is not always the case in practice.

- question: How does the Naive Bayes perform with categorical variables?
  answer: > 
    It performs well with categorical variables since no assumpion is made on
    the data distribution. By contrast with numerical variables a normal distribution
    is assumed.

- h3: Regression
  question: What do you predict with linear regression?
  answer: Linear regression predicts a real value

- question: What are the most common techniques used for computing the coefficients in linear regression?
  answer: >
    The most common techniques used for computing the coefficients in linear regression are 
    <br><br> - <b>Ordinary Least Squares</b> seeks to minimize the sum of the squared residuals, i.e. the distance 
    between the points and the regression line.
    <br><br> - <b>Gradient descendent</b> works by inizializing randomly the coefficients. A learning rate is 
    used as a scale factor and the coefficients are updated in the direction towards minimizing the error. 
    The process is repeated until a minimum sum squared error is achieved or no further improvement is 
    possible.

- question: What is logistic regression?
  answer: >
    It is a statistical technique that is used to analyze a dataset and predict the binary outcome. 
    The outcome has to be a binary outcome that is either zero or one or a yes or no. 
    In other words the output of a logistic regression is always categorical (= discrete)


- question: What do you predict with logistic regression?
  answer: >
    Logistic regression predicts the probabilty that a variable belongs to one 
    class or another. 

- question: Can you explain a real use case for logistic regression?
  answer: >
    You can use logistic regression to predict the probability that a customer will buy 
    a product.	

- question: How can you turn logistic regression into a classifier?
  answer: >
    To turn logistic regression into a classifier you can use a threshold. If the 
    output of the logistic regression is above (below) the threshold, then the output
    can be classified as class zero (class one).

- h3: K-Nearest neighbor algorithm (KNN)
  question: What is the KNN algorithm?
  answer: >
    A k-nearest neighbor (KNN) method scores an example by finding the k training examples 
    nearest to the example and then taking the average of their outcomes as the score.

- question: What are some advantages and disadvantages of the KNN algorithm?
  answer: >
    Some advantages are
    <br>- simple concept
    <br>- builiding a model is cheap
    <br>- no assumption is made about the data distrubution
    <br><br>Some disadvantages are
    <br>- classifying unknown records is expensive, in particular with big data sets, 
         since you have to find the K nearest neighbors
    <br>- pretty affected by data missing and data scaling

- question: Make an example of a real user-case for the KNN algorithm
  answer: >
    You can use the KNN algorithm to recommend a product to a customer 
    based on the purchases of other similar customers.
    

- h3: Decision trees
  question: What is a decision tree?
  answer: >
    A <b>decision tree</b> is a supervised machine learning algorithm that generates a flowchart from a dataset. This flowchart
    can be represented as a tree-like structure. You can also think of a decision tree as a sequence of if-else conditions where a
    a feature is evaluated at each if-else condition.
    <br><br>
    For example, the decision tree in the picture below models the question of going to the beach. 
    According to this decision tree, we go to the beach only when the weather is sunny and the temperature is higher than 25 °C.
  image: decision_tree.PNG
- question: How are decision trees built?
  answer: >
    <b>Building a decision tree</b> is a recursive process. 
    At each step the algorithm searches for the best possible feature that can be used to split the dataset while minimizing the
    entropy within each subset. The subsets are then split repeatedly into even smaller subsets, and so on and so forth until 
    the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another 
    stopping criterion has been met.
    <br><br>
    The <b>entropy</b> describes the variety in a dataset. An entropy of 0 means that all items are in the same class. The 
    maximum entropy possible is when each item is in a different class.  
    <br><br>This algorithm is a greedy algorithm since it chooses a local optimal solution at each step.
- question: What are the main advantages of decision trees?
  answer: >
    The <b>main advantages of decision trees</b> are:
    <ul>
    <li><b>Output simple to understand</b>: the output tree is human readable, i.e. it can be interpreted easily. 
    This makes decision trees particularly interesting for cases 
    where the decision mechanism needs to be transparent (for example in medical diagnostics).</li>
    <li><b>Minimum data preparation</b>: decision trees do require less data cleaning and data preparation. 
    Indeed they do not need data normalization/scaling and
    they can work well even with missing values.</li>
    </ul>
- question: What are some disadvantages of decision trees?
  answer: >
    Some <b>disadvantages of decision trees</b> are:
    <ul>
    <li><b>Overfitting</b>: decision trees can easily overfit the data.</li>
    <li><b>Instability</b>: small changes in the input data can lead to a very different tree output structure.</li>
    <li><b>Complex data structure</b>: the structure of decision trees can become complex for example when the dataset has a lot
    of features.</li>
    </ul>
- question: >
    What technique can you use to reduce the overfitting of a decision tree? How many approaches are there?
  answer: >
    To <b>reduce the overfitting of a decision tree</b> you can use <b>pruning</b>, which reduces the size of the tree. 
    There are two approaches to pruning<br>
    <ul>
    <li><b>Pre-pruning</b>: the tree stop growing when it reaches a certain number of decisions or when 
    the decision nodes contains too few items. A disadvantage of this approach is that some important 
    data could be pruned.</li>
    <li><b>Post-pruning</b>: the tree can grow as needed and only at the end it is checked if the tree 
    is too big. This approach guarantees that the important data is not pruned.</li>
    </ul>
- question: >
   What are the main techniques used to combine decision trees to obtain a more accurate model 
   (so called ensemble techniques)?
  answer: >
    There are two main ensemble techniques
    <ul>
    <li><b>Boosting algorithms</b>: this is a sequential process. In boosting algorithms learners 
    are learned sequentially with early learners fitting simple models to the data and then analyzing 
    data for errors. After each training step the weights are redistributed. 
    Misclassified data gets an increased weight so that this data will be more on focus in the 
    next train step.</li>
    <li><b>Bagging techniques</b>: this is a parallel process. In a bagging technique the dataset is 
    divided into n samples using randomized sampling.Then a model is build on each sample. 
    After that the resulting models are combined using voting or averaging.</li>
    </ul>
    
- h3: Support Vector Machine
  question: What is Support Vector Machine and what is its main idea?
  answer: >
    <b>Support Vector Machine</b> (SVM) is a supervised machine learning algorithm that tries to find high dimensional
    planes that divide a dataset into clusters. The planes are found in such a way that they are as far as possible from any point.
    <br><br>
    The position of the planes depends on the so called <b>support vectors</b>, that are the points that are closest to the planes.
    
- question: What is the technique called <i>kerneling</i> that is used by Support Vector Machine?
  answer:
    Under the hood, SVM uses a technique called <b>kerneling</b> to find clusters that might not be apparent in lower
    dimensions. More specifically, the dataset is mapped via kernel functions into a higher dimension where the
    data is potentially easier to cluster.

- question: What is Support Vector Machine particulary good at? What are some disadvantages? 
  answer: >
      Support Vector Machine is particularly good at classifying high-dimensional data, i.e. data with a lot of features.
      <br><br>
      SVM has some disadvantages like:
      <ul>
        <li>it does not perform well with large datasets because of high computational costs.</li>
        <li>the kernel must be chosen carefully and this can be tricky.</li>
      </ul> 
