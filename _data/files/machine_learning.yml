
- h2: Machine learning

  question: What is the difference between data science and machine learning
  answer: >
    Data science = statistical analysis + data engineering in order to get 
    information from the data
    <br><br>
    Machine learning = create a model to predict something. You can also define 
    it as teaching computers to learn by examples

- question: What are the typical steps of a machine Learning process?
  answer: > 
    The typical steps of a machine learning process are
    <br>- domain knowledge
    <br>- feature selection
    <br>- choose machine learning algorithm
    <br>- training
    <br>- evaluation
    <br>- optimization
    <br>- testing
    <br>- deployment
   
- question: >
    All machine learning algorithms work essentially in a similar way. What is it?
  answer: >
    All machine learning algorithms work essentially like
    <br>- make a prediction
    <br>- compute the error given by (predicted values - real values)
    <br>- make a new prediction that reduces the error above
    <br>- repeat until the error is minimized

- question: What is overfitting and what are some techniques for preventing it?
  answer: >
    Overfitting happens when your model works well with the training dataset but
    does not work well with new data, which means the model cannot generalize well. 
    <br>
    To prevent overfitting you can for example use regularization and cross-validation.

- question: What is cross-validation in machine learning?
  answer: > 
    Cross-validation refers to the process of splitting a dataset into a 
    training set (usually 80%) and test set (usually 20%) and evaluate
    the performance of the model with those two datasets. 

- question: What is K-fold cross-validation?
  answer: >
    K-fold cross-validation is a kind of cross-validation. The steps are
    <br>- split a dataset into k parts. Each part is called fold.
    <br>- take one fold i as test set and take the remaining (k-1) folds as training set.
    <br>- train you classification algorithm with the two datasets from previous point. Finally for 
    each fold we get an accuracy value. i.e. in total we get K accuracy values.
    <br>- the final accuracy value for your classification algorithm is given by the average of
    the K accuracy values.

- question: What is regularization?
  answer: >
    Regularization adds a penalty on the different parameters of the model to reduce the 
    freedom of the model. Regularization is usually done by adding a correction term to 
    the formula for a mathematical model. This correction term penalizes the higher order 
    terms so that the model is forced to be simpler.

- question: What is the curse of dimensionality?
  answer: >
    The curse of dimensionality is a tendency that it is easier to overfit a dataset 
    when there are few points and many features. Data needs to increase exponentially 
    with the number of features in order not to have overfitting.

- question: What is unsupervised learning? Make also some examples of algorithms
  answer: > 
    Unsupervised learning is a machine learning process with unlabeled data.
    <br><br>
    Some examples of unsupervised learning are
    <br> - Clustering algorithms (K-means, hierarchical custering, Probabilistic clustering)
    <br> - Dimension reduction algorithms (PCA, Single Value Decomposition SVD)

- question: What is reinforcement learning?
  answer: >
    It is a machine learning algorithm that receives feedback on his output so that the accuracy
    of the output is improved based on this feedback. Finally the algorithm learns through trial
    and error.

- question: What is supervised learning?
  answer: It is a machine learning process with a labeled training dataset.

- question: What are the two main types of supervised learning algorithms?
  answer: >
    Supervised machine learning algorithms are mainly classified 
    into classification and regression.

- question: Can you mention some supervised machine learning algorithms?
  answer: >
    The most common supervised machine learning algorithms are 
    <br>K-nearest neighbor, Naive Bayes, Decision Trees, Linear Regression, 
    Support Vector Machines.

- question: What are hyperparameters?
  answer: >
    Hyperparameters are parameters used by the training model and 
    they must be obviously set before starting the training process. 
    An example is the the k value in the k-nearest neighbor algorithm.

- h3: Evaluate machine learning methods
  question: What is the difference between model evaluation and model validation?
  answer: >
    The difference between model evaluation and model validation is that
    <br><br>model evaluation is used to check the performance of a machine learning model
    on the training set.
    <br><br>model validation is used to ensure that a model performs well also on a test set.

  question: What is bias in a machine learning model?
  answer: >
    Bias is the error that is introduced by analyzing a complex model using a too simple model. 
    In other words bias is the inability of the model to capture the real nature of the true 
    relationship in the data.

- question: What does high bias mean for a machine learning model?
  answer: High bias means the model is underfitting the data.

- question: What is the variance of a machine learning model?
  answer: > 
    Variance is the amount that the estimate of the target function will change if different training 
    data was used.

- question: What does high variance mean for a machine learning model?
  answer: > 
    High variance means the model is overfitting, ie small changes in the training data set lead 
    to big changes in the output of the model.

- question: What matrix do you construct to evaluate the performance of a classifier?
  answer: >
    To evaluate the performance of a classifier you construct a confusion matrix
    <table>
      <tr>
        <td></td>
        <th>y = 1</th> 
        <th>y = -1</th>
      </tr>
      <tr>
        <td>y# = 1</td>
        <td>TP</td> 
        <td>FP</td>
      </tr>
      <tr>
        <td>y# = -1</td>
        <td>FN</td> 
        <td>TN</td>
      </tr>
    </table>
    <br>
    where 
    y = actual label, y# = predicted label, TP = true positives, FP = false positives, FN = false negatives
    TN = true negatives.
    <br><br>
    True positives (negatives) = number of items with label y = 1 (-1) that 
    are classified correctly.
    <br><br>
    False positives (negatives) = number of items with labels y = -1 (1) that
    are classified wrongly.

- question: Can you mention some metrics used to evaluate classifiers?
  answer: >
    Accuracy = (TP + TN) / n
    <br>true positive rate or sentitivity  or recall = TP / (TP + FN)
    <br>true negative rate or specificity = TN / (TN + FP)
    <br>Precision  = TP / (TP + FP)
    <br>False positive rate (FPR)  = FP / (TN + FP)
    <br>False  negative rate (FNR)  = FN / (TP + FN)
    <br><br>
    <b>Accuracy</b> is the percentage of the correctly classified items.
    <br><br>
    <b>Precision</b> is the percentage of the items classified as positive that are actually positive
    <br><br>
    <b>Sensitivity</b> or <b>Recall</b> is the percentage of the positive items that
    were actually classified as positive.
    <br><br>
    <b>Specificity</b> is the percentage of the negative items that
    were actually classified as negative.

- question: > 
    What metrics are more meaningful with balanced datasets and what work better for unbalanced datasets?
    Balanced (unbalanced) dataset means you (do not) have the same number of samples for each class. 
  answer: >
    When the dataset is balanced, the accuracy is a good metric. 
    <br><br>If the dataset is unbalanced, precision and recall
    are better metrices. 

- question: To evaluate a generic machine learning model what metrics do you typically use?
  answer: > 
    You would probably use the misclassification error or accuracy. 
    However, notice that accuracy works well when all classes are equally important and the 
    training dataset is balanced.

- question: > 
    To evaluate a machine learning model for cancer detection what metrics do 
    doctors typically use?
  answer: > 
    Doctors would use sensitivity and specificity because the cost of a misclassified cancer 
    is much higher than the cost of a healthy tissue classified as cancer. In addition a typical
    cancer dataset is strongly unbalanced, with few cancel samples and many healthy samples.

- question: What is the meaning and use of the F-measure (also called F-score)?
  answer: >
    The F-measure combines the precision and recall using the harmonic mean.
    <br><br>
    F-measure = 2 (Precision X Recall) / (Precision + Recall)
    <br><br>
    The F-measure value is between 0 (worst case) and 1 (best case). It can be used
    when you want to compare the performance of two classifiers w.r.t. precision and recall
    but you are not sure which of the two values to pick. So with the F-measure you seek
    a balance between precision and recall.



- question: If you want to evaluate a classifier with a curve what do you use?
  answer: >
    You can use an ROC curve. The ROC curve plots the sensitivity (= TPF) against
    1-specificity (=FPR) for different values of a parameter that affects the classifier.
    <br><br>
    Sensitivity = probability of predicting that a real positive will be positive. This can
    be seen as the accuracy of positives.
    <br>
    Specificity = probability of predicting that a real negative will be negative. This can
    be seen as the accuracy of negatives.
    <br><br>
    The best result is high sensitivity and low specificity which means the closest the curve
    is to point (0,1), the better. This can be also expressed as the farther the curve is
    from the diagonal line (0,0)-(1,1) the better. This diagonal line represents the line of 
    a classifier working at random. 
    <br>
    The distance of the curve from point (0,1) can be measured with the AUC (area under the curve)
    that should be close to 1 as much as possible.

    
    answers to the question 
    “for a particular false positive rate what is the corresponding true positive rate?
  image: roc_curve.png

- h3: Naive Bayes
  question: What is Naive Bayes?
  answer: >
    Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem that is used
    to solve classification problems.

- question: Why is Naive Bayes called "naive"?
  answer: >
    Because the Naive Bayes is based on the idea that the predictor variables are independent of 
    each other while in nature this is oft not the case.

- question: What is the Bayes theorem?
  answer: >
    The Bayes theorem is a mathematical formula for finding P(A|B) from P(B|A).
    The formula is 
    <br><br>
    P(A|B) = P(B|A) * P(A) / P(B)


- question: What are the main advantages of Naive Bayes?
  answer: >
    The Naive Bayes is a relatively simple and very quick algorithm since it is a probabilistic 
    model that does not require a training step. This makes it very scalable.

- question: When is the Naive Bayes usually used?
  answer: > 
    Naive Bayes is often used in <br>
    - text classification like spam filter<br>
    - real-time classification since it is fast<br>
    - multi-class prediction

- question: When Naive Bayes is used with numerical variables, what condition is assumed on the data?
  answer: >
    Numerical variables used in Naive Bayes are expected to have a normal distribution.
    This is not always the case in practice.

- question: How does the Naive Bayes perform with categorical variables?
  answer: > 
    It performs well with categorical variables since no assumpion is made on
    the data distribution. By contrast with numerical variables a normal distribution
    is assumed.

- h3: Regression
  question: What do you predict with linear regression?
  answer: Linear regression predicts a real value

- question: What are the most common techniques used for computing the coefficients in linear regression?
  answer: >
    The most common techniques used for computing the coefficients in linear regression are 
    <br> - Ordinary Least Squares. Seeks to minimize the sum of the squared residuals, ie the distance 
    between the points and the regression line.
    <br><br> - Gradient descendent. Works by inizializing randomly the coefficients. A learning rate is 
    used as a scale factor and the coefficients are updated in the direction towards minimizing the error. 
    The process is repeated until a minimum sum squared error is achieved or no further improvement is 
    possible.

- question: What is logistic regression?
  answer: >
    It is a statistical technique that is used to analyze a dataset and predict the binary outcome. 
    The outcome has to be a binary outcome that is either zero or one or a yes or no. 
    In other words the output of a logistic regression is always categorical (= discrete)


- question: What do you predict with logistic regression?
  answer: >
    Logistic regression predicts the probabilty that a variable belongs to one 
    class or another. 

- question: Can you explain a real use case for logistic regression?
  answer: >
    You can use logistic regression to predict the probability that a customer will buy 
    a product.	

- question: How can you turn logistic regression into a classifier?
  answer: >
    To turn logistic regression into a classifier you can use a threshold. If the 
    output of the logistic regression is above (below) the threshold, then the output
    can be classified as class zero (class one).

- h3: K-Nearest neighbor algorithm (KNN)
  question: What is the KNN algorithm?
  answer: >
    A k-nearest neighbor (KNN) method scores an example by finding the k training examples 
    nearest to the example and then taking the average of their outcomes as the score.

- question: What are some advantages and disadvantages of the KNN algorithm?
  answer: >
    Some advantages are
    <br>- simple concept
    <br>- builiding a model is cheap
    <br>- no assumption is made about the data distrubution
    <br><br>Some disadvantages are
    <br>- classifying unknown records is expensive, in particular with big data sets, 
         since you have to find the K nearest neighbors
    <br>- pretty affected by data missing and data scaling

- question: Make an example of a real user-case for the KNN algorithm
  answer: >
    You can use the KNN algorithm to recommend a product to a customer 
    based on the purchases of other similar customers.
    

- h3: Decision trees
  question: What is a decision tree?
  answer: >
    A decision tree is a classification algorithm that models the features and the data in a tree-like 
    structure.
- question: How are decision trees built?
  answer: >
    Building a decision tree is a recursive process. The dataset is split into subsets, which are 
    then split repeatedly into even smaller subsets, and so on and so forth until the process stops 
    when the algorithm determines the data within the subsets are sufficiently homogenous, or another 
    stopping criterion has been met.
- question: What is a great advantage of using decision trees?
  answer: >
    The output of the decision trees is human readable. This makes decision trees interesting to be 
    used in cases where the decision mechanism needs to be transparent because of legal reasons 
    (for example medical diagnostics)
- question: What are some disadvantages of decision trees?
  answer: >
    - each split considers one feature at a time which means that the decision lines are basically 
    parallel to the axis and are not diagonal, which would be the case if you would evaluate two or 
    more features within the same split.
    <br><br>- it is easy to overfit or underfit the model
    <br><br>- small changes in the input data will lead to a very different output
- question: >
    What technique do you use to reduce the overfitting of a decision tree? How many approaches there are?
  answer: >
    To reduce the overfitting of a decision tree you can use pruning, which reduces the size of the tree. 
    There are two approaches to pruning<br>
    <br><br>Pre-pruning - the tree stop growing when it reaches a certain number of decisions or when 
    the decision nodes contains too few items. A disadvantage of this approach is that some important 
    data could be pruned.
    <br><br>Post-pruning - the tree can grow as needed and only at the end it is checked if the tree 
    is too big. This approach guarantees that the important data is not pruned.

- question: >
   What are the main techniques to combine decision trees to obtain a more accurate model 
   (so called ensemble techniques)?
  answer: >
    There are two techniques
    <br><br> Boosting algorithms. This is a sequential process. In boosting algorithms learners 
    are learned sequentially with early learners fitting simple models to the data and then analyzing 
    data for errors. After each training step the weights are redistributed. 
    Misclassified data gets an increased weight so that this data will be more on focus in the 
    next train step.
    <br><br>Bagging techniques. This is a parallel process. In a bagging technique the dataset is 
    divided into n samples using randomized sampling.Then a model is build on each sample. 
    After that the resulting models are combined using voting or averaging.



